{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Leakage Fix Verification\n",
    "\n",
    "This notebook demonstrates the train/test split evaluation methodology that prevents data leakage in time series forecasting.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "1. **Split data**: 80% train, 20% test\n",
    "2. **Evaluate**: Train ONLY on train data, evaluate on test (unseen) data\n",
    "3. **Production**: Retrain on ALL data for final forecast\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "- **Old approach**: Model could see future data during walk-forward validation\n",
    "- **New approach**: True out-of-sample evaluation on completely unseen data\n",
    "- **Result**: Honest performance metrics + best production model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from gw2ml.data.loaders import load_gw2_series\n",
    "from gw2ml.evaluation.backtest import walk_forward_backtest\n",
    "from gw2ml.modeling.arima import ARIMAModel\n",
    "from gw2ml.modeling.xgboost import XGBoostModel\n",
    "from gw2ml.metrics.registry import get_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Item ID: 19702\n",
      "  Days back: 3\n",
      "  Train/Test split: 80% / 20%\n",
      "  Forecast horizon: 60 steps (used for both backtest and production)\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "#¬†19702 - Platinum Ore, 19976 - Mystic Coin\n",
    "ITEM_ID = 19702  # GW2 item ID to analyze\n",
    "DAYS_BACK = 3  # Days of historical data\n",
    "VALUE_COLUMN = \"buy_unit_price\"\n",
    "FORECAST_HORIZON = 24  # Steps ahead for both evaluation and production forecast\n",
    "TRAIN_SPLIT = 0.8  # 80% train, 20% test\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Item ID: {ITEM_ID}\")\n",
    "print(f\"  Days back: {DAYS_BACK}\")\n",
    "print(f\"  Train/Test split: {TRAIN_SPLIT:.0%} / {(1-TRAIN_SPLIT):.0%}\")\n",
    "print(f\"  Forecast horizon: {FORECAST_HORIZON} steps (used for both backtest and production)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data\n",
    "\n",
    "Load historical price data for the item. This will be cached locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "‚úì Loaded 864 data points\n",
      "  Time range: 2026-01-02 14:40:00 to 2026-01-05 14:35:00\n",
      "  Value range: 155.00 to 199.00\n"
     ]
    }
   ],
   "source": [
    "# Load data (uses caching)\n",
    "print(\"Loading data...\")\n",
    "series_meta = load_gw2_series(\n",
    "    item_id=ITEM_ID,\n",
    "    days_back=DAYS_BACK,\n",
    "    value_column=VALUE_COLUMN,\n",
    "    fill_missing_dates=True,\n",
    ")\n",
    "\n",
    "series = series_meta.series\n",
    "print(f\"‚úì Loaded {len(series)} data points\")\n",
    "print(f\"  Time range: {series.start_time()} to {series.end_time()}\")\n",
    "print(f\"  Value range: {series.values().min():.2f} to {series.values().max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Split Data into Train/Test\n",
    "\n",
    "**Critical**: The model will NEVER see test data during evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split:\n",
      "  Train: 691 points (80.0%)\n",
      "  Test:  173 points (20.0%)\n",
      "\n",
      "Train period: 2026-01-02 14:40:00 to 2026-01-05 00:10:00\n",
      "Test period:  2026-01-05 00:15:00 to 2026-01-05 14:35:00\n"
     ]
    }
   ],
   "source": [
    "# Split into train/test\n",
    "split_idx = int(len(series) * TRAIN_SPLIT)\n",
    "train_series = series[:split_idx]\n",
    "test_series = series[split_idx:]\n",
    "\n",
    "print(f\"Data split:\")\n",
    "print(f\"  Train: {len(train_series)} points ({len(train_series)/len(series):.1%})\")\n",
    "print(f\"  Test:  {len(test_series)} points ({len(test_series)/len(series):.1%})\")\n",
    "print(f\"\\nTrain period: {train_series.start_time()} to {train_series.end_time()}\")\n",
    "print(f\"Test period:  {test_series.start_time()} to {test_series.end_time()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Evaluate ARIMA on Test Set\n",
    "\n",
    "Train ONLY on train data, evaluate on held-out test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ARIMA model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212c771b0c2647f0af40afd2fcbb3e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "historical forecasts:   0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì ARIMA Results:\n",
      "  Test set MAPE: 1.9251%\n",
      "  Forecast points: 114\n",
      "  This is TRUE out-of-sample performance!\n"
     ]
    }
   ],
   "source": [
    "# ARIMA evaluation\n",
    "print(\"Training ARIMA model...\")\n",
    "arima_params = {\"p\": 1, \"d\": 1, \"q\": 1, \"seasonal_order\": None}\n",
    "\n",
    "arima_result = walk_forward_backtest(\n",
    "    model_class=ARIMAModel,\n",
    "    model_params=arima_params,\n",
    "    series=series,\n",
    "    train_series=train_series,  # Train on this ONLY\n",
    "    test_series=test_series,    # Evaluate on this\n",
    "    forecast_horizon=FORECAST_HORIZON,\n",
    "    stride=1,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Calculate metrics\n",
    "mape_fn = get_metric(\"mape\")\n",
    "arima_mape = mape_fn(arima_result.actuals, arima_result.forecasts)\n",
    "\n",
    "print(f\"\\n‚úì ARIMA Results:\")\n",
    "print(f\"  Test set MAPE: {arima_mape:.4f}%\")\n",
    "print(f\"  Forecast points: {len(arima_result.forecasts)}\")\n",
    "print(f\"  This is TRUE out-of-sample performance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visualize Walk-Forward Rolling Window\n",
    "\n",
    "This shows how the model is evaluated using a rolling window approach:\n",
    "- **Train on past data** (expanding window)\n",
    "- **Predict N steps ahead** (forecast horizon)\n",
    "- **Move forward** by stride steps\n",
    "- **Repeat** until test set is covered\n",
    "\n",
    "This ensures the model NEVER sees future data during evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert TimeSeries to DataFrame\n",
    "def to_df(ts):\n",
    "    df = ts.to_dataframe()\n",
    "    df.index = pd.to_datetime(df.index).tz_localize(None)\n",
    "    return df\n",
    "\n",
    "# Visualize the rolling window walk-forward process\n",
    "print(f\"Walk-Forward Backtest Details:\")\n",
    "print(f\"  Total data points: {len(series)}\")\n",
    "print(f\"  Train set: {len(train_series)} points\")\n",
    "print(f\"  Test set: {len(test_series)} points\")\n",
    "print(f\"  Forecast horizon: {FORECAST_HORIZON} steps\")\n",
    "print(f\"  Stride: {arima_result.stride} step\")\n",
    "\n",
    "# Convert to dataframes first\n",
    "train_df = to_df(train_series)\n",
    "test_df = to_df(test_series)\n",
    "actual_df = test_df\n",
    "\n",
    "# Build full forecast windows for visualization\n",
    "viz_model = ARIMAModel(**arima_params)\n",
    "viz_model.fit(train_series)\n",
    "combined_series = train_series.append(test_series)\n",
    "\n",
    "window_forecasts = viz_model.historical_forecasts(\n",
    "    series=combined_series,\n",
    "    start=len(train_series),\n",
    "    forecast_horizon=FORECAST_HORIZON,\n",
    "    stride=arima_result.stride,\n",
    "    retrain=True,\n",
    "    last_points_only=False,\n",
    ")\n",
    "\n",
    "if not isinstance(window_forecasts, list):\n",
    "    window_forecasts = [window_forecasts]\n",
    "\n",
    "window_dfs = [to_df(wf) for wf in window_forecasts]\n",
    "num_windows = len(window_dfs)\n",
    "\n",
    "print(f\"  Number of forecast windows: {num_windows}\")\n",
    "\n",
    "if num_windows:\n",
    "    print(\"\")\n",
    "    print(\"Forecast coverage:\")\n",
    "    print(f\"  First window: {window_dfs[0].index[0]} -> {window_dfs[0].index[-1]}\")\n",
    "    print(f\"  Last window (with actuals): {window_dfs[-1].index[0]} -> {window_dfs[-1].index[-1]}\")\n",
    "    print(f\"  Train ends: {train_df.index[-1]}\")\n",
    "    print(f\"  Test starts: {test_df.index[0]}\")\n",
    "\n",
    "# Determine time step for the extra window\n",
    "if len(test_df.index) > 1:\n",
    "    step = test_df.index[1] - test_df.index[0]\n",
    "else:\n",
    "    step = pd.Timedelta(hours=1)\n",
    "\n",
    "extra_window_start = test_df.index[-1] + step\n",
    "extra_window_end = extra_window_start + step * (FORECAST_HORIZON - 1)\n",
    "\n",
    "# Optional: production-style forecast after test data ends\n",
    "production_forecast_df = None\n",
    "production_model = ARIMAModel(**arima_params)\n",
    "production_model.fit(series)\n",
    "production_forecast_df = to_df(production_model.predict(FORECAST_HORIZON))\n",
    "\n",
    "# Create visualization\n",
    "fig = go.Figure()\n",
    "\n",
    "# Show EVERY Nth window to demonstrate the rolling pattern\n",
    "if num_windows:\n",
    "    step_size = max(1, num_windows // 20)\n",
    "\n",
    "    # Add subtle gradient showing all windows\n",
    "    for idx in range(0, num_windows, step_size):\n",
    "        wf_df = window_dfs[idx]\n",
    "        window_start = wf_df.index[0]\n",
    "        window_end = wf_df.index[-1]\n",
    "\n",
    "        # Gradient from red (start) to blue (end)\n",
    "        progress = idx / max(1, num_windows - 1)\n",
    "        red = int(255 * (1 - progress))\n",
    "        blue = int(255 * progress)\n",
    "        color = f\"rgba({red}, 100, {blue}, 0.05)\"\n",
    "        border_color = f\"rgba({red}, 100, {blue}, 0.25)\"\n",
    "\n",
    "        # Add very subtle shaded region\n",
    "        fig.add_vrect(\n",
    "            x0=window_start,\n",
    "            x1=window_end,\n",
    "            fillcolor=color,\n",
    "            layer=\"below\",\n",
    "            line_width=0.5,\n",
    "            line_dash=\"dot\",\n",
    "            line_color=border_color,\n",
    "        )\n",
    "\n",
    "    # Highlight 3 specific windows for reference\n",
    "    highlight_indices = [0, num_windows // 2, max(0, num_windows - 1)]\n",
    "    highlight_colors = [\"rgba(255, 0, 0, 0.15)\", \"rgba(0, 255, 0, 0.15)\", \"rgba(0, 100, 255, 0.15)\"]\n",
    "    border_colors = [\"red\", \"green\", \"blue\"]\n",
    "    labels = [\"W1\", \"W2\", \"W3\"]\n",
    "\n",
    "    for idx, color, border_color, label in zip(highlight_indices, highlight_colors, border_colors, labels):\n",
    "        if idx < len(window_dfs):\n",
    "            wf_df = window_dfs[idx]\n",
    "            window_start = wf_df.index[0]\n",
    "            window_end = wf_df.index[-1]\n",
    "\n",
    "            # Highlighted window\n",
    "            fig.add_vrect(\n",
    "                x0=window_start,\n",
    "                x1=window_end,\n",
    "                fillcolor=color,\n",
    "                layer=\"below\",\n",
    "                line_width=2,\n",
    "                line_dash=\"dash\",\n",
    "                line_color=border_color,\n",
    "            )\n",
    "\n",
    "            # Label\n",
    "            mid_idx = len(wf_df) // 2\n",
    "            fig.add_annotation(\n",
    "                x=wf_df.index[mid_idx], y=0.98, yref=\"paper\",\n",
    "                text=label,\n",
    "                showarrow=False,\n",
    "                font=dict(size=10, color=border_color, family=\"Arial Black\"),\n",
    "                bgcolor=\"rgba(255,255,255,0.9)\",\n",
    "                bordercolor=border_color,\n",
    "                borderwidth=2,\n",
    "            )\n",
    "\n",
    "# Add one extra window after test data ends\n",
    "fig.add_vrect(\n",
    "    x0=extra_window_start,\n",
    "    x1=extra_window_end,\n",
    "    fillcolor=\"rgba(120, 120, 120, 0.08)\",\n",
    "    layer=\"below\",\n",
    "    line_width=1,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"rgba(120, 120, 120, 0.6)\",\n",
    ")\n",
    "\n",
    "extra_mid = extra_window_start + (extra_window_end - extra_window_start) / 2\n",
    "fig.add_annotation(\n",
    "    x=extra_mid, y=0.98, yref=\"paper\",\n",
    "    text=\"Next (no actuals)\",\n",
    "    showarrow=False,\n",
    "    font=dict(size=10, color=\"rgba(80, 80, 80, 1)\", family=\"Arial Black\"),\n",
    "    bgcolor=\"rgba(255,255,255,0.9)\",\n",
    "    bordercolor=\"rgba(120, 120, 120, 0.6)\",\n",
    "    borderwidth=2,\n",
    ")\n",
    "\n",
    "# Add train/test split line\n",
    "fig.add_vline(\n",
    "    x=train_df.index[-1],\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"black\",\n",
    "    line_width=2,\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "    x=train_df.index[-1], y=0.5, yref=\"paper\",\n",
    "    text=\"Train/Test<br>Split\", showarrow=True, arrowhead=2,\n",
    "    ax=40, ay=0,\n",
    "    font=dict(size=11, color=\"black\"),\n",
    "    bgcolor=\"rgba(255,255,255,0.9)\",\n",
    "    bordercolor=\"black\",\n",
    "    borderwidth=2,\n",
    ")\n",
    "\n",
    "# Add the actual data lines ON TOP\n",
    "# Train data (gray)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=train_df.index,\n",
    "    y=train_df.iloc[:, 0],\n",
    "    mode=\"lines\",\n",
    "    name=\"Train Data\",\n",
    "    line=dict(color=\"gray\", width=2),\n",
    "    opacity=0.6,\n",
    "))\n",
    "\n",
    "# Forecast windows (orange)\n",
    "forecast_legend_shown = False\n",
    "highlight_map = {\n",
    "    0: \"W1\",\n",
    "    num_windows // 2: \"W2\",\n",
    "    max(0, num_windows - 1): \"W3\",\n",
    "} if num_windows else {}\n",
    "\n",
    "for idx, wf_df in enumerate(window_dfs):\n",
    "    if idx in highlight_map:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=wf_df.index,\n",
    "            y=wf_df.iloc[:, 0],\n",
    "            mode=\"lines\",\n",
    "            name=f\"Forecast {highlight_map[idx]}\",\n",
    "            line=dict(color=\"#ff7f0e\", width=2.5),\n",
    "        ))\n",
    "    else:\n",
    "        showlegend = not forecast_legend_shown\n",
    "        if showlegend:\n",
    "            forecast_legend_shown = True\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=wf_df.index,\n",
    "            y=wf_df.iloc[:, 0],\n",
    "            mode=\"lines\",\n",
    "            name=\"Forecast Window\",\n",
    "            showlegend=showlegend,\n",
    "            line=dict(color=\"rgba(255, 127, 14, 0.2)\", width=1),\n",
    "        ))\n",
    "\n",
    "# Test data actual (blue, thick)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=actual_df.index,\n",
    "    y=actual_df.iloc[:, 0],\n",
    "    mode=\"lines\",\n",
    "    name=\"Test Actual\",\n",
    "    line=dict(color=\"#1f77b4\", width=3),\n",
    "))\n",
    "\n",
    "# Production forecast (after data ends)\n",
    "if production_forecast_df is not None:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=production_forecast_df.index,\n",
    "        y=production_forecast_df.iloc[:, 0],\n",
    "        mode=\"lines\",\n",
    "        name=\"Production Forecast\",\n",
    "        line=dict(color=\"#2f2f2f\", width=2, dash=\"dot\"),\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Walk-Forward Validation: {FORECAST_HORIZON}-Step Rolling Windows\",\n",
    "    xaxis_title=\"Time\",\n",
    "    yaxis_title=f\"{VALUE_COLUMN}\",\n",
    "    hovermode=\"x unified\",\n",
    "    height=500,\n",
    "    showlegend=True,\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\")\n",
    "print(\"üìä Interpretation:\")\n",
    "print(\"  ‚Ä¢ GRAY: Training data (model never trained on test data)\")\n",
    "print(\"  ‚Ä¢ BLUE: Actual test prices (ground truth)\")\n",
    "print(\"  ‚Ä¢ ORANGE: Forecast windows (each forecast only shown within its test window)\")\n",
    "print(f\"  ‚Ä¢ Shaded boxes: Rolling {FORECAST_HORIZON}-step windows starting at the split\")\n",
    "print(\"  ‚Ä¢ Dashed box: One extra window after test data ends\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Production Forecast (Retrain on ALL Data)\n",
    "\n",
    "After evaluation, retrain on ALL data for the best production forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì MAPE: 2.4613% (4032 points, 80.9s)\n",
      "  Testing 21 days of data... "
     ]
    }
   ],
   "source": [
    "# Test different amounts of historical data to find the sweet spot\n",
    "import time\n",
    "\n",
    "# Test different days_back values\n",
    "days_to_test = [1, 2, 3, 5, 7, 10, 14, 21, 30]\n",
    "results = []\n",
    "\n",
    "print(\"Finding ARIMA's Sweet Spot - Testing different training window sizes...\")\n",
    "print(f\"Testing with forecast horizon: {FORECAST_HORIZON} steps\\n\")\n",
    "\n",
    "for days in days_to_test:\n",
    "    print(f\"  Testing {days} days of data...\", end=\" \")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Load data with this window size\n",
    "        test_series_meta = load_gw2_series(\n",
    "            item_id=ITEM_ID,\n",
    "            days_back=days,\n",
    "            value_column=VALUE_COLUMN,\n",
    "            fill_missing_dates=True,\n",
    "        )\n",
    "        \n",
    "        # Split into train/test (80/20)\n",
    "        test_series_obj = test_series_meta.series\n",
    "        split_idx = int(len(test_series_obj) * TRAIN_SPLIT)\n",
    "        train_subset = test_series_obj[:split_idx]\n",
    "        test_subset = test_series_obj[split_idx:]\n",
    "        \n",
    "        # Skip if not enough test data\n",
    "        if len(test_subset) < FORECAST_HORIZON:\n",
    "            print(\"‚ö†Ô∏è  Not enough test data, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Run backtest\n",
    "        backtest_result = walk_forward_backtest(\n",
    "            model_class=ARIMAModel,\n",
    "            model_params=arima_params,\n",
    "            series=test_series_obj,\n",
    "            train_series=train_subset,\n",
    "            test_series=test_subset,\n",
    "            forecast_horizon=FORECAST_HORIZON,\n",
    "            stride=1,\n",
    "            verbose=False,\n",
    "        )\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mape = mape_fn(backtest_result.actuals, backtest_result.forecasts)\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        results.append({\n",
    "            'days': days,\n",
    "            'data_points': len(test_series_obj),\n",
    "            'train_points': len(train_subset),\n",
    "            'test_points': len(test_subset),\n",
    "            'mape': mape,\n",
    "            'time_seconds': elapsed,\n",
    "            'forecasts': len(backtest_result.forecasts)\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úì MAPE: {mape:.4f}% ({len(test_series_obj)} points, {elapsed:.1f}s)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Failed: {str(e)[:50]}\")\n",
    "        continue\n",
    "\n",
    "# Create visualization\n",
    "if results:\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        subplot_titles=(\n",
    "            \"ARIMA Accuracy vs Training Data Size\",\n",
    "            \"Training Time vs Data Size\"\n",
    "        ),\n",
    "        vertical_spacing=0.15,\n",
    "    )\n",
    "    \n",
    "    # Top plot: MAPE vs days\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[r['days'] for r in results],\n",
    "        y=[r['mape'] for r in results],\n",
    "        mode='lines+markers',\n",
    "        name='MAPE',\n",
    "        line=dict(width=3, color='#1f77b4'),\n",
    "        marker=dict(size=10),\n",
    "    ), row=1, col=1)\n",
    "    \n",
    "    # Find the elbow point (where improvement slows down)\n",
    "    if len(results) >= 3:\n",
    "        # Simple heuristic: point where MAPE improvement is < 5% of previous improvement\n",
    "        improvements = []\n",
    "        for i in range(1, len(results)):\n",
    "            prev_mape = results[i-1]['mape']\n",
    "            curr_mape = results[i]['mape']\n",
    "            improvement = prev_mape - curr_mape\n",
    "            improvements.append(improvement)\n",
    "        \n",
    "        # Find sweet spot (diminishing returns)\n",
    "        sweet_spot_idx = 0\n",
    "        for i in range(1, len(improvements)):\n",
    "            if improvements[i] < improvements[i-1] * 0.3:  # Less than 30% of previous improvement\n",
    "                sweet_spot_idx = i\n",
    "                break\n",
    "        \n",
    "        if sweet_spot_idx > 0:\n",
    "            sweet_spot = results[sweet_spot_idx]\n",
    "            fig.add_vline(\n",
    "                x=sweet_spot['days'],\n",
    "                line_dash=\"dash\",\n",
    "                line_color=\"red\",\n",
    "                annotation_text=f\"Sweet Spot: {sweet_spot['days']} days\",\n",
    "                annotation_position=\"top\",\n",
    "                row=1, col=1\n",
    "            )\n",
    "    \n",
    "    # Bottom plot: Time vs days\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[r['days'] for r in results],\n",
    "        y=[r['time_seconds'] for r in results],\n",
    "        mode='lines+markers',\n",
    "        name='Training Time',\n",
    "        line=dict(width=3, color='#ff7f0e'),\n",
    "        marker=dict(size=10),\n",
    "    ), row=2, col=1)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Days of Historical Data\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Days of Historical Data\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"MAPE (%)\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Time (seconds)\", row=2, col=1)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=700,\n",
    "        showlegend=False,\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"{'Days':<6} {'Points':<8} {'MAPE':<10} {'Time':<8} {'Notes'}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    best_mape = min(results, key=lambda r: r['mape'])\n",
    "    fastest = min(results, key=lambda r: r['time_seconds'])\n",
    "    \n",
    "    for r in results:\n",
    "        notes = []\n",
    "        if r == best_mape:\n",
    "            notes.append(\"‚≠ê Best MAPE\")\n",
    "        if r == fastest:\n",
    "            notes.append(\"‚ö° Fastest\")\n",
    "        if sweet_spot_idx > 0 and r == results[sweet_spot_idx]:\n",
    "            notes.append(\"üéØ Sweet Spot\")\n",
    "        \n",
    "        print(f\"{r['days']:<6} {r['data_points']:<8} {r['mape']:<10.4f} {r['time_seconds']:<8.1f} {' | '.join(notes)}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Recommendation:\")\n",
    "    if sweet_spot_idx > 0:\n",
    "        rec = results[sweet_spot_idx]\n",
    "        print(f\"   Use {rec['days']} days of data ({rec['data_points']} points)\")\n",
    "        print(f\"   - MAPE: {rec['mape']:.4f}%\")\n",
    "        print(f\"   - Training time: {rec['time_seconds']:.1f}s\")\n",
    "        print(f\"   - Balance between accuracy and speed\")\n",
    "    else:\n",
    "        print(f\"   Use {best_mape['days']} days for best accuracy\")\n",
    "        print(f\"   - MAPE: {best_mape['mape']:.4f}%\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No successful tests - unable to determine sweet spot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS: Find ARIMA's Sweet Spot\n",
    "\n",
    "How much historical data does ARIMA need for optimal performance? Let's test different training window sizes to find the point of diminishing returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Complete Picture\n",
    "\n",
    "Shows the full workflow:\n",
    "1. Train data\n",
    "2. Test evaluation\n",
    "3. Production forecast (trained on all data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Compare with XGBoost (Optional)\n",
    "\n",
    "Run the same evaluation for XGBoost to compare models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2199e35514fc4830843876d96ccfd97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "historical forecasts:   0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì XGBoost Results:\n",
      "  Test set MAPE: 2.2176%\n",
      "\n",
      "üìä Model Comparison:\n",
      "  ARIMA MAPE:   1.8693%\n",
      "  XGBoost MAPE: 2.2176%\n",
      "  Best model: ARIMA\n"
     ]
    }
   ],
   "source": [
    "# XGBoost evaluation\n",
    "print(\"Training XGBoost model...\")\n",
    "xgb_params = {\"lags\": 12, \"n_estimators\": 100}\n",
    "\n",
    "try:\n",
    "    xgb_result = walk_forward_backtest(\n",
    "        model_class=XGBoostModel,\n",
    "        model_params=xgb_params,\n",
    "        series=series,\n",
    "        train_series=train_series,\n",
    "        test_series=test_series,\n",
    "        forecast_horizon=FORECAST_HORIZON,\n",
    "        stride=1,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    xgb_mape = mape_fn(xgb_result.actuals, xgb_result.forecasts)\n",
    "    print(f\"\\n‚úì XGBoost Results:\")\n",
    "    print(f\"  Test set MAPE: {xgb_mape:.4f}%\")\n",
    "    \n",
    "    # Compare models\n",
    "    print(f\"\\nüìä Model Comparison:\")\n",
    "    print(f\"  ARIMA MAPE:   {arima_mape:.4f}%\")\n",
    "    print(f\"  XGBoost MAPE: {xgb_mape:.4f}%\")\n",
    "    winner = \"ARIMA\" if arima_mape < xgb_mape else \"XGBoost\"\n",
    "    print(f\"  Best model: {winner}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è XGBoost failed: {e}\")\n",
    "    print(\"  (This might be due to MPS/GPU incompatibility)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### ‚úÖ What We Did\n",
    "\n",
    "1. **Split data** into 80% train / 20% test\n",
    "2. **Trained** models ONLY on train data\n",
    "3. **Evaluated** on held-out test data (never seen during training)\n",
    "4. **Retrained** on ALL data for production forecasts\n",
    "\n",
    "### üõ°Ô∏è Why This Prevents Data Leakage\n",
    "\n",
    "- Model **never sees** test data during evaluation phase\n",
    "- Test metrics represent **true out-of-sample** performance\n",
    "- Production model uses **all available data** for best forecasts\n",
    "\n",
    "### üìà Results\n",
    "\n",
    "- **Test set MAPE**: Honest performance metric\n",
    "- **Plots show**: Clear separation between train/test/forecast\n",
    "- **No flat sections**: Real predictions at every time point\n",
    "\n",
    "This methodology ensures your models are evaluated honestly and your production forecasts use all available information!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}